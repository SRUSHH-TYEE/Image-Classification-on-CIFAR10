{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a74987e1",
   "metadata": {},
   "source": [
    "### HW2 -  Linear Classifier\n",
    "### Srushti Nayak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7b849",
   "metadata": {},
   "source": [
    "#### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b359ccea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528e0557",
   "metadata": {},
   "source": [
    "#### Loading CIFAR-10 data from pytorch and applying transformation on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b5f37a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b165f9b",
   "metadata": {},
   "source": [
    "#### Converting to numppy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f8e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([item[0].numpy().flatten() for item in trainset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ece330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd6cc448",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([item[1] for item in trainset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c07e7f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1be76a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([item[0].numpy().flatten() for item in testset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9d5f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([item[1] for item in testset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be16fa",
   "metadata": {},
   "source": [
    "#### Implementing linear classifier without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e780e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_classifier:\n",
    "    \n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def setHyperParameters(self, epochs, learning_rate, total_classes, input_size):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_size = input_size\n",
    "        self.weights = np.random.randn(input_size, total_classes) * 0.0001\n",
    "        self.bias = np.zeros((1, total_classes))\n",
    "        \n",
    "    def forward_pass(self, X, w, bias):\n",
    "        return np.dot(X, w) + bias\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "         \n",
    "    def cross_entropy_loss(self, y_pred, y_train):\n",
    "        m = y_train.shape[0]\n",
    "        log_likelihood = -np.log(y_pred[range(m), y_train])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward_pass(self, X, y_train, y_pred):\n",
    "        m = y_train.shape[0]\n",
    "        grad_softmax = y_pred\n",
    "        grad_softmax[range(m), y_train] -= 1\n",
    "        grad_softmax /= m\n",
    "        grad_weights = np.dot(X.T, grad_softmax)\n",
    "        grad_bias = np.sum(grad_softmax, axis=0, keepdims=True)\n",
    "        return grad_weights, grad_bias\n",
    "\n",
    "    def update_parameters(self, grad_weights, grad_bias):\n",
    "        self.weights = self.weights - learning_rate * grad_weights\n",
    "        self.bias = self.bias - learning_rate * grad_bias\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            output = self.forward_pass(self.X_train, self.weights,self.bias)\n",
    "            y_pred = self.softmax(output)\n",
    "            cross_entropy_loss = self.cross_entropy_loss(y_pred, self.y_train)\n",
    "            grad_weights, grad_bias = self.backward_pass(self.X_train, self.y_train, y_pred)\n",
    "            self.update_parameters(grad_weights, grad_bias)\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch + 1}, Loss: {cross_entropy_loss}')\n",
    "    \n",
    "    def test(self, X_test, y_test):\n",
    "        \n",
    "        output = self.forward_pass(X_test, self.weights, self.bias)\n",
    "        y_pred = self.softmax(output)\n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "        accuracy = np.mean(predictions == y_test) * 100 \n",
    "        print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e9781",
   "metadata": {},
   "source": [
    "#### defining hyperparameters and applying linear regression to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ae8789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "input_size = 32*32*3\n",
    "total_classes = 10\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1776f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "liner_classifier_without_regularization = Linear_classifier(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0769c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "liner_classifier_without_regularization.setHyperParameters(epochs, learning_rate, total_classes, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e59320e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.0969283844936224\n",
      "Epoch 20, Loss: 2.024564744811901\n",
      "Epoch 30, Loss: 1.982178246335534\n",
      "Epoch 40, Loss: 1.9529024317851003\n",
      "Epoch 50, Loss: 1.9310551652515895\n",
      "Epoch 60, Loss: 1.9139478214258445\n",
      "Epoch 70, Loss: 1.9000892960581703\n",
      "Epoch 80, Loss: 1.8885683058601854\n",
      "Epoch 90, Loss: 1.8787905710346968\n",
      "Epoch 100, Loss: 1.8703501588161369\n"
     ]
    }
   ],
   "source": [
    "liner_classifier_without_regularization.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0badc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.59\n"
     ]
    }
   ],
   "source": [
    "liner_classifier_without_regularization.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fbb10f",
   "metadata": {},
   "source": [
    "#### Linear classifier with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b18a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_classifier_L1_reg:\n",
    "    \n",
    "    # initializing trainset\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def setHyperParameters(self, epochs, learning_rate, total_classes, l1_strength, input_size):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_size = input_size\n",
    "        self.l1_strength = l1_strength\n",
    "        self.weights = np.random.randn(input_size, total_classes) * 0.0001\n",
    "        self.bias = np.zeros((1, total_classes))\n",
    "        \n",
    "    def forward_pass(self, X, w):\n",
    "        return np.dot(X, w) + self.bias\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "         \n",
    "    def cross_entropy_loss(self, y_pred, y_train):\n",
    "        m = y_train.shape[0]\n",
    "        log_likelihood = -np.log(y_pred[range(m), y_train])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward_pass(self, X, y_train, y_pred):\n",
    "        m = y_train.shape[0]\n",
    "        grad_softmax = y_pred\n",
    "        grad_softmax[range(m), y_train] -= 1\n",
    "        grad_softmax /= m\n",
    "        grad_weights = np.dot(X.T, grad_softmax)\n",
    "        grad_bias = np.sum(grad_softmax, axis=0, keepdims=True)\n",
    "        return grad_weights, grad_bias\n",
    "\n",
    "    def update_parameters(self, grad_weights, grad_bias):\n",
    "        self.weights = self.weights - (self.learning_rate * grad_weights)\n",
    "        self.bias = self.bias - (self.learning_rate * grad_bias)\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            output = self.forward_pass(self.X_train, self.weights)\n",
    "            y_pred = self.softmax(output)\n",
    "            l1_regularization = self.l1_strength * np.sum(np.abs(self.weights))\n",
    "            total_loss = self.cross_entropy_loss(y_pred, self.y_train) + l1_regularization\n",
    "            grad_weights, grad_bias = self.backward_pass(self.X_train, self.y_train, y_pred)\n",
    "            grad_weights = grad_weights + ((self.l1_strength/self.X_train.shape[0]) * np.sign(self.weights))\n",
    "            self.update_parameters(grad_weights, grad_bias)\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch + 1}, Loss: {total_loss}')\n",
    "    \n",
    "    def test(self, X_test, y_test):\n",
    "        \n",
    "        output = self.forward_pass(X_test, self.weights)\n",
    "        y_pred = self.softmax(output)\n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "        accuracy = np.mean(predictions == y_test) * 100 \n",
    "        print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb12cc3d",
   "metadata": {},
   "source": [
    "#### defining hyperparameters and applying linear regression to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82847010",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "input_size = 32*32*3\n",
    "total_classes = 10\n",
    "learning_rate = 0.01\n",
    "l1_strength = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53c8c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_regularizated_model = Linear_classifier_L1_reg(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13d840b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_regularizated_model.setHyperParameters(epochs, learning_rate, total_classes, l1_strength, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26be14ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.1148008933834697\n",
      "Epoch 20, Loss: 2.0532408211226745\n",
      "Epoch 30, Loss: 2.0192267699225233\n",
      "Epoch 40, Loss: 1.9968820119349238\n",
      "Epoch 50, Loss: 1.9809770471509422\n",
      "Epoch 60, Loss: 1.9690880822570616\n",
      "Epoch 70, Loss: 1.95988775667998\n",
      "Epoch 80, Loss: 1.9525712915296314\n",
      "Epoch 90, Loss: 1.9466317200885108\n",
      "Epoch 100, Loss: 1.941723356799024\n"
     ]
    }
   ],
   "source": [
    "L1_regularizated_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45bc8b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.63\n"
     ]
    }
   ],
   "source": [
    "L1_regularizated_model.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b76fcb",
   "metadata": {},
   "source": [
    "#### Linear classifier with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faa29506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_classifier_L2_reg:\n",
    "    \n",
    "    # initializing trainset\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def setHyperParameters(self, epochs, learning_rate, total_classes, l2_strength, input_size):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_size = input_size\n",
    "        self.l2_strength = l2_strength\n",
    "        self.weights = np.random.randn(input_size, total_classes) * 0.0001\n",
    "        self.bias = np.zeros((1, total_classes))\n",
    "        \n",
    "    def forward_pass(self, X, w):\n",
    "        return np.dot(X, w) + self.bias\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "         \n",
    "    def cross_entropy_loss(self, y_pred, y_train):\n",
    "        m = y_train.shape[0]\n",
    "        log_likelihood = -np.log(y_pred[range(m), y_train])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss \n",
    "    \n",
    "    def backward_pass(self, X, y_train, y_pred):\n",
    "        m = y_train.shape[0]\n",
    "        grad_softmax = y_pred\n",
    "        grad_softmax[range(m), y_train] -= 1\n",
    "        grad_softmax /= m\n",
    "        grad_weights = np.dot(X.T, grad_softmax)\n",
    "        grad_bias = np.sum(grad_softmax, axis=0, keepdims=True)\n",
    "        return grad_weights, grad_bias\n",
    "\n",
    "    def update_parameters(self, grad_weights, grad_bias):\n",
    "        self.weights = self.weights - learning_rate * grad_weights\n",
    "        self.bias = self.bias - learning_rate * grad_bias\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            output = self.forward_pass(self.X_train, self.weights)\n",
    "            y_pred = self.softmax(output)\n",
    "            l2_regularization = 0.5 * self.l2_strength * np.sum(self.weights ** 2)\n",
    "            total_loss = self.cross_entropy_loss(y_pred, self.y_train) + l2_regularization\n",
    "            grad_weights, grad_bias = self.backward_pass(self.X_train, self.y_train, y_pred)\n",
    "            grad_weights = grad_weights + ((self.l2_strength/self.X_train.shape[0]) * self.weights)\n",
    "            self.update_parameters(grad_weights, grad_bias)\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch + 1}, Loss: {total_loss}')\n",
    "    \n",
    "    def test(self, X_test, y_test):\n",
    "        \n",
    "        output = self.forward_pass(X_test, self.weights)\n",
    "        y_pred = self.softmax(output)\n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "        accuracy = np.mean(predictions == y_test) * 100 \n",
    "        print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b5425",
   "metadata": {},
   "source": [
    "#### defining hyperparameters and applying linear regression to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65008f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "input_size = 32*32*3\n",
    "total_classes = 10\n",
    "learning_rate = 0.01\n",
    "l2_strength = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a12aff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_regularizated_model = Linear_classifier_L2_reg(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d17ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_regularizated_model.setHyperParameters(epochs, learning_rate, total_classes, l2_strength, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4843395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.096936524143008\n",
      "Epoch 20, Loss: 2.024587661917383\n",
      "Epoch 30, Loss: 1.9822163851384886\n",
      "Epoch 40, Loss: 1.9529554578706227\n",
      "Epoch 50, Loss: 1.9311229142298898\n",
      "Epoch 60, Loss: 1.9140300549419642\n",
      "Epoch 70, Loss: 1.9001857024903595\n",
      "Epoch 80, Loss: 1.8886785288519177\n",
      "Epoch 90, Loss: 1.8789142335273004\n",
      "Epoch 100, Loss: 1.870486880134562\n"
     ]
    }
   ],
   "source": [
    "L2_regularizated_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e623039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.65\n"
     ]
    }
   ],
   "source": [
    "L2_regularizated_model.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d72d5",
   "metadata": {},
   "source": [
    "#### Linear classifier with Elastic net regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bb91374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_classifier_Elastic_net_reg:\n",
    "    \n",
    "    # initializing trainset\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def setHyperParameters(self, epochs, learning_rate, total_classes, l1_strength, l2_strength, input_size):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_size = input_size\n",
    "        self.l2_strength = l2_strength\n",
    "        self.l1_strength = l1_strength\n",
    "        self.weights = np.random.randn(input_size, total_classes) * 0.0001\n",
    "        self.bias = np.zeros((1, total_classes))\n",
    "        \n",
    "    def forward_pass(self, X, w):\n",
    "        return np.dot(X, w) + self.bias\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "         \n",
    "    def cross_entropy_loss(self, y_pred, y_train):\n",
    "        m = y_train.shape[0]\n",
    "        log_likelihood = -np.log(y_pred[range(m), y_train])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward_pass(self, X, y_train, y_pred):\n",
    "        m = y_train.shape[0]\n",
    "        grad_softmax = y_pred\n",
    "        grad_softmax[range(m), y_train] -= 1\n",
    "        grad_softmax /= m\n",
    "        grad_weights = np.dot(X.T, grad_softmax)\n",
    "        grad_bias = np.sum(grad_softmax, axis=0, keepdims=True)\n",
    "        return grad_weights, grad_bias\n",
    "\n",
    "    def update_parameters(self, grad_weights, grad_bias):\n",
    "        self.weights = self.weights - learning_rate * grad_weights\n",
    "        self.bias = self.bias - learning_rate * grad_bias\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            output = self.forward_pass(self.X_train, self.weights)\n",
    "            y_pred = self.softmax(output)\n",
    "            l1_regularization = self.l1_strength * np.sum(np.abs(self.weights))\n",
    "            l2_regularization = 0.5 * self.l2_strength * np.sum(self.weights ** 2)\n",
    "            total_loss = self.cross_entropy_loss(y_pred, self.y_train) + l2_regularization + l1_regularization\n",
    "            grad_weights, grad_bias = self.backward_pass(self.X_train, self.y_train, y_pred)\n",
    "            grad_weights = grad_weights + (self.l2_strength * self.weights) + ((self.l1_strength/self.X_train.shape[0]) * np.sign(self.weights))\n",
    "            self.update_parameters(grad_weights, grad_bias)\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch + 1}, Loss: {total_loss}')\n",
    "    \n",
    "    def test(self, X_test, y_test):\n",
    "        \n",
    "        output = self.forward_pass(X_test, self.weights)\n",
    "        y_pred = self.softmax(output)\n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "        accuracy = np.mean(predictions == y_test) * 100 \n",
    "        print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e1e78e",
   "metadata": {},
   "source": [
    "#### defining hyperparameters and applying linear regression to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47315f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "input_size = 32*32*3\n",
    "total_classes = 10\n",
    "learning_rate = 0.01\n",
    "l2_strength = 0.001\n",
    "l1_strength = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b8ab051",
   "metadata": {},
   "outputs": [],
   "source": [
    "Elastic_net_regularizated_model = Linear_classifier_Elastic_net_reg(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ef3c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Elastic_net_regularizated_model.setHyperParameters(epochs, learning_rate, total_classes, l2_strength, l1_strength, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12427968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.1148000638974196\n",
      "Epoch 20, Loss: 2.053295150721024\n",
      "Epoch 30, Loss: 2.019308280281109\n",
      "Epoch 40, Loss: 1.9969778161678378\n",
      "Epoch 50, Loss: 1.981089256932737\n",
      "Epoch 60, Loss: 1.9692172078041092\n",
      "Epoch 70, Loss: 1.9600370963981186\n",
      "Epoch 80, Loss: 1.9527441137377186\n",
      "Epoch 90, Loss: 1.9468193032607963\n",
      "Epoch 100, Loss: 1.9419235910178454\n"
     ]
    }
   ],
   "source": [
    "Elastic_net_regularizated_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d79508c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.68\n"
     ]
    }
   ],
   "source": [
    "Elastic_net_regularizated_model.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3e3c9",
   "metadata": {},
   "source": [
    "#### 1)Hyperparameter tuning, learning_rate = 0.05 and l2_strength =0.001 epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a900f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "input_size = 32*32*3\n",
    "total_classes = 10\n",
    "learning_rate = 0.05\n",
    "l2_strength = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2beadb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_regularizated_model_2 = Linear_classifier_L2_reg(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8abb496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_regularizated_model_2.setHyperParameters(epochs, learning_rate, total_classes, l2_strength, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c7732df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.9361919787675559\n",
      "Epoch 20, Loss: 1.8723947896384951\n",
      "Epoch 30, Loss: 1.841754304692046\n",
      "Epoch 40, Loss: 1.8223956863764237\n",
      "Epoch 50, Loss: 1.8084357610026562\n",
      "Epoch 60, Loss: 1.7975690183705746\n",
      "Epoch 70, Loss: 1.788681322988357\n",
      "Epoch 80, Loss: 1.7811591264769988\n",
      "Epoch 90, Loss: 1.7746328780047926\n",
      "Epoch 100, Loss: 1.7688647263313106\n"
     ]
    }
   ],
   "source": [
    "L2_regularizated_model_2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b461b1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.6\n"
     ]
    }
   ],
   "source": [
    "L2_regularizated_model_2.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400045d0",
   "metadata": {},
   "source": [
    "#### 2_1) Hyperparameter tuning, learning_rate = 0.05 and l2_strength =0.001 epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ec6caa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "input_size = 32*32*3\n",
    "total_classes = 10\n",
    "learning_rate = 0.05\n",
    "l2_strength = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "074625b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_regularizated_model_2_1 = Linear_classifier_L2_reg(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "356a3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_regularizated_model_2_1.setHyperParameters(epochs, learning_rate, total_classes, l2_strength, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ca86b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.9361753654664873\n",
      "Epoch 20, Loss: 1.872394581535456\n",
      "Epoch 30, Loss: 1.8417618540048166\n",
      "Epoch 40, Loss: 1.8224074415429055\n",
      "Epoch 50, Loss: 1.8084497416993404\n",
      "Epoch 60, Loss: 1.7975840462129833\n",
      "Epoch 70, Loss: 1.788696693459349\n",
      "Epoch 80, Loss: 1.7811744232040736\n",
      "Epoch 90, Loss: 1.7746478610494063\n",
      "Epoch 100, Loss: 1.768879264042551\n",
      "Epoch 110, Loss: 1.763707480546505\n",
      "Epoch 120, Loss: 1.7590186095170117\n",
      "Epoch 130, Loss: 1.754729310639533\n",
      "Epoch 140, Loss: 1.7507767990843897\n",
      "Epoch 150, Loss: 1.7471125804434444\n",
      "Epoch 160, Loss: 1.7436983807172737\n",
      "Epoch 170, Loss: 1.7405034179002796\n",
      "Epoch 180, Loss: 1.7375025226245693\n",
      "Epoch 190, Loss: 1.7346748125827662\n",
      "Epoch 200, Loss: 1.7320027377257265\n"
     ]
    }
   ],
   "source": [
    "L2_regularizated_model_2_1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4e3dd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.6\n"
     ]
    }
   ],
   "source": [
    "L2_regularizated_model_2_1.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0734a7a",
   "metadata": {},
   "source": [
    "#### 2) Hyperparameter tuning, learning_rate = 0.05 and l2_strength =0.008 and epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1a79f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "input_size = 32*32*3\n",
    "total_classes = 10\n",
    "learning_rate = 0.05\n",
    "l2_strength = 0.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25e4c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_regularizated_model_3 = Linear_classifier_L2_reg(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c9a7623",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_regularizated_model_3.setHyperParameters(epochs, learning_rate, total_classes, l2_strength, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52753ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.936637503615806\n",
      "Epoch 20, Loss: 1.873325840745382\n",
      "Epoch 30, Loss: 1.8431068627271214\n",
      "Epoch 40, Loss: 1.8241232683914383\n",
      "Epoch 50, Loss: 1.8105059930280765\n",
      "Epoch 60, Loss: 1.799958913488967\n",
      "Epoch 70, Loss: 1.7913741142084367\n",
      "Epoch 80, Loss: 1.7841422551623467\n",
      "Epoch 90, Loss: 1.7778967017406386\n",
      "Epoch 100, Loss: 1.7724016709256067\n",
      "Epoch 110, Loss: 1.7674974435192408\n",
      "Epoch 120, Loss: 1.7630711858325718\n",
      "Epoch 130, Loss: 1.7590403666308236\n",
      "Epoch 140, Loss: 1.7553428236284554\n",
      "Epoch 150, Loss: 1.7519305476274996\n",
      "Epoch 160, Loss: 1.748765646955843\n",
      "Epoch 170, Loss: 1.7458176436723336\n",
      "Epoch 180, Loss: 1.7430616121539892\n",
      "Epoch 190, Loss: 1.7404768668387118\n",
      "Epoch 200, Loss: 1.738046017477062\n"
     ]
    }
   ],
   "source": [
    "L2_regularizated_model_3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63b76ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.61\n"
     ]
    }
   ],
   "source": [
    "L2_regularizated_model_3.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e8dc66",
   "metadata": {},
   "source": [
    "#### 3) Hyperparameter tuning, learning_rate = 0.08 and l2_strength =0.01 and epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90578979",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "input_size = 32*32*3\n",
    "total_classes = 10\n",
    "learning_rate = 0.08\n",
    "l2_strength = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "432439d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_regularizated_model_4 = Linear_classifier_L2_reg(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "757f5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_regularizated_model_4.setHyperParameters(epochs, learning_rate, total_classes, l2_strength, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd4bcb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.901802214386546\n",
      "Epoch 20, Loss: 1.8550643236504945\n",
      "Epoch 30, Loss: 1.8290737455960873\n",
      "Epoch 40, Loss: 1.8103163782946268\n",
      "Epoch 50, Loss: 1.7963182569481493\n",
      "Epoch 60, Loss: 1.7852497149049797\n",
      "Epoch 70, Loss: 1.7761013539701003\n",
      "Epoch 80, Loss: 1.7683074841829214\n",
      "Epoch 90, Loss: 1.7615273081604375\n",
      "Epoch 100, Loss: 1.7555413679066474\n",
      "Epoch 110, Loss: 1.7502003692576185\n",
      "Epoch 120, Loss: 1.7453981455111067\n",
      "Epoch 130, Loss: 1.741056481223965\n",
      "Epoch 140, Loss: 1.737116102300407\n",
      "Epoch 150, Loss: 1.7335309962269119\n",
      "Epoch 160, Loss: 1.7302645581880718\n",
      "Epoch 170, Loss: 1.7272867342811016\n",
      "Epoch 180, Loss: 1.724571725333625\n",
      "Epoch 190, Loss: 1.7220960913212373\n",
      "Epoch 200, Loss: 1.7198373104088787\n"
     ]
    }
   ],
   "source": [
    "L2_regularizated_model_4.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6bbcd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.92\n"
     ]
    }
   ],
   "source": [
    "L2_regularizated_model_4.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62140d1b",
   "metadata": {},
   "source": [
    "#### 4) Hyperparameter tuning, learning_rate = 0.1 and l2_strength =0.05 and epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f10bc3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "input_size = 32*32*3\n",
    "total_classes = 10\n",
    "learning_rate = 0.1\n",
    "l2_strength = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73c84cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_regularizated_model_5 = Linear_classifier_L2_reg(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a91d6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_regularizated_model_5.setHyperParameters(epochs, learning_rate, total_classes, l2_strength, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "90235082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.8246386100282188\n",
      "Epoch 20, Loss: 1.8570471974176934\n",
      "Epoch 30, Loss: 1.8245493234876502\n",
      "Epoch 40, Loss: 1.8078264983792118\n",
      "Epoch 50, Loss: 1.7962322791288885\n",
      "Epoch 60, Loss: 1.7954034446238525\n",
      "Epoch 70, Loss: 1.902244401422413\n",
      "Epoch 80, Loss: 1.8658989217080193\n",
      "Epoch 90, Loss: 1.7941933048065102\n",
      "Epoch 100, Loss: 1.772268532131215\n",
      "Epoch 110, Loss: 1.783651673834355\n",
      "Epoch 120, Loss: 1.8356816885906109\n",
      "Epoch 130, Loss: 1.8119042252531996\n",
      "Epoch 140, Loss: 1.7684065392860866\n",
      "Epoch 150, Loss: 1.7669647907771806\n",
      "Epoch 160, Loss: 1.7748381850478219\n",
      "Epoch 170, Loss: 1.7849427019694726\n",
      "Epoch 180, Loss: 1.7843237191553127\n",
      "Epoch 190, Loss: 1.765706149143236\n",
      "Epoch 200, Loss: 1.7600607788086509\n"
     ]
    }
   ],
   "source": [
    "L2_regularizated_model_4.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "862ac9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.51\n"
     ]
    }
   ],
   "source": [
    "L2_regularizated_model_4.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979b722",
   "metadata": {},
   "source": [
    "#### 5) Hyperparameter tuning, learning_rate = 0.08 and l2_strength =0.09 and epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69458287",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "input_size = 32*32*3\n",
    "total_classes = 10\n",
    "learning_rate = 0.08\n",
    "l2_strength = 0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fce22f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_regularizated_model_6 = Linear_classifier_L2_reg(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5476bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_regularizated_model_6.setHyperParameters(epochs, learning_rate, total_classes, l2_strength, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74420753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.9100935298664912\n",
      "Epoch 20, Loss: 1.8711670331859431\n",
      "Epoch 30, Loss: 1.851710456634327\n",
      "Epoch 40, Loss: 1.8387760607322887\n",
      "Epoch 50, Loss: 1.830166057522058\n",
      "Epoch 60, Loss: 1.8242019168782653\n",
      "Epoch 70, Loss: 1.819962113804135\n",
      "Epoch 80, Loss: 1.8169346505178983\n",
      "Epoch 90, Loss: 1.8148130533040514\n",
      "Epoch 100, Loss: 1.8134006457226546\n",
      "Epoch 110, Loss: 1.8125637149925546\n",
      "Epoch 120, Loss: 1.8122070098064997\n",
      "Epoch 130, Loss: 1.8122601163742944\n",
      "Epoch 140, Loss: 1.812669440623826\n",
      "Epoch 150, Loss: 1.813393184654593\n",
      "Epoch 160, Loss: 1.8143979376223067\n",
      "Epoch 170, Loss: 1.815656124497563\n",
      "Epoch 180, Loss: 1.8171439201613862\n",
      "Epoch 190, Loss: 1.8188394971812998\n",
      "Epoch 200, Loss: 1.8207216810723585\n"
     ]
    }
   ],
   "source": [
    "L2_regularizated_model_6.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2d3fd482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.93\n"
     ]
    }
   ],
   "source": [
    "L2_regularizated_model_6.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89428ca1",
   "metadata": {},
   "source": [
    "#### 6) Hyperparameter tuning, learning_rate = 0.08 and epoch = 200 with linear model without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59847eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "input_size = 32*32*3\n",
    "total_classes = 10\n",
    "learning_rate = 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa5536c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "liner_classifier_without_regularization_2 = Linear_classifier(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "45d3a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "liner_classifier_without_regularization_2.setHyperParameters(epochs, learning_rate, total_classes, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "adaa92fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.9006792823785386\n",
      "Epoch 20, Loss: 1.8529873925410258\n",
      "Epoch 30, Loss: 1.8262369641104448\n",
      "Epoch 40, Loss: 1.8067591584518823\n",
      "Epoch 50, Loss: 1.7920873318135002\n",
      "Epoch 60, Loss: 1.7803801088251001\n",
      "Epoch 70, Loss: 1.7706177206064813\n",
      "Epoch 80, Loss: 1.7622277578933252\n",
      "Epoch 90, Loss: 1.7548650865599427\n",
      "Epoch 100, Loss: 1.7483073643548652\n",
      "Epoch 110, Loss: 1.7424033223320514\n",
      "Epoch 120, Loss: 1.7370454089021956\n",
      "Epoch 130, Loss: 1.7321544177569679\n",
      "Epoch 140, Loss: 1.7276703528415394\n",
      "Epoch 150, Loss: 1.7235466659463494\n",
      "Epoch 160, Loss: 1.719746346972083\n",
      "Epoch 170, Loss: 1.7162390289512828\n",
      "Epoch 180, Loss: 1.7129986656608094\n",
      "Epoch 190, Loss: 1.710001618001369\n",
      "Epoch 200, Loss: 1.7072252003900252\n"
     ]
    }
   ],
   "source": [
    "liner_classifier_without_regularization_2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d681c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.94\n"
     ]
    }
   ],
   "source": [
    "liner_classifier_without_regularization_2.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21288f66",
   "metadata": {},
   "source": [
    "#### 7) Hyperparameter tuning, l1_strength = 0.001, learning_rate = 0.08 and epoch = 200 with linear model with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53039fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "input_size = 32*32*3\n",
    "total_classes = 10\n",
    "learning_rate = 0.08\n",
    "l1_strength = 0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "03c82d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_regularizated_model_7 = Linear_classifier_L1_reg(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "88ed2c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_regularizated_model_7.setHyperParameters(epochs, learning_rate, total_classes, l1_strength, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41b03b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 7.535921929769115\n",
      "Epoch 20, Loss: 9.714207088083983\n",
      "Epoch 30, Loss: 11.11025620830869\n",
      "Epoch 40, Loss: 12.162764538332011\n",
      "Epoch 50, Loss: 13.032807217147104\n",
      "Epoch 60, Loss: 13.790142721257883\n",
      "Epoch 70, Loss: 14.469212084720864\n",
      "Epoch 80, Loss: 15.08987181198009\n",
      "Epoch 90, Loss: 15.666066347674036\n",
      "Epoch 100, Loss: 16.206818606348314\n",
      "Epoch 110, Loss: 16.71837019335496\n",
      "Epoch 120, Loss: 17.205722078815285\n",
      "Epoch 130, Loss: 17.672739161643282\n",
      "Epoch 140, Loss: 18.122158010637058\n",
      "Epoch 150, Loss: 18.555660278514193\n",
      "Epoch 160, Loss: 18.975647332672075\n",
      "Epoch 170, Loss: 19.38346993202391\n",
      "Epoch 180, Loss: 19.779862791013663\n",
      "Epoch 190, Loss: 20.164886754020817\n",
      "Epoch 200, Loss: 20.53945465643307\n"
     ]
    }
   ],
   "source": [
    "L1_regularizated_model_7.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa8fd136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.89\n"
     ]
    }
   ],
   "source": [
    "L1_regularizated_model_7.test(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
