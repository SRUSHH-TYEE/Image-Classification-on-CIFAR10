{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34d54e78",
   "metadata": {},
   "source": [
    "### HW3 2-Layer NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c7f78",
   "metadata": {},
   "source": [
    "### Srushti Nayak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c4bbf7",
   "metadata": {},
   "source": [
    "#### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4b68343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd5cf080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ce7a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([item[0].numpy().flatten() for item in trainset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a8b9fd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5bf98745",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([item[1] for item in trainset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7a0da7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0c11563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([item[0].numpy().flatten() for item in testset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "347ea72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([item[1] for item in testset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3ae22",
   "metadata": {},
   "source": [
    "#### Implementing 2-layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "caf06014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid withought regularization\n",
    "class NNSig_without_reg:\n",
    "    \n",
    "    def __init__(self, x_train,y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = 128\n",
    "        \n",
    "    def setHyperParameters(self, input_size, hidden_size, output_size, learning_rate, epochs):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_size = input_size\n",
    "        self.w1 = np.random.randn(input_size, hidden_size)\n",
    "        self.w2 = np.random.randn(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        hidden_layer_input = x_batch.dot(self.w1) \n",
    "        hidden_layer_output = 1 / (1 + np.exp(-hidden_layer_input))\n",
    "        output_layer_input = hidden_layer_output.dot(self.w2) \n",
    "        scores = np.exp(output_layer_input - np.max(output_layer_input, axis=1, keepdims=True))\n",
    "\n",
    "        return hidden_layer_input, hidden_layer_output, scores\n",
    "\n",
    "    def loss(self,y_batch,scores):\n",
    "        correct_scores = scores[np.arange(len(scores)), y_batch]\n",
    "        loss = -np.sum(np.log(correct_scores / np.sum(scores, axis=1)))\n",
    "        return loss\n",
    "        \n",
    "    def backward(self, hidden_layer_input, hidden_layer_output, scores, y_batch,x_batch):\n",
    "        grad_scores = scores / np.sum(scores, axis=1, keepdims=True)\n",
    "        grad_scores[np.arange(len(grad_scores)), y_batch] -= 1\n",
    "        grad_scores /= len(grad_scores)\n",
    "        \n",
    "        grad_w2 = hidden_layer_output.T.dot(grad_scores)\n",
    "        grad_hidden = grad_scores.dot(self.w2.T)\n",
    "        \n",
    "        grad_hidden_layer_input = grad_hidden * (hidden_layer_output * (1 - hidden_layer_output))\n",
    "        grad_w1 = x_batch.T.dot(grad_hidden_layer_input)\n",
    "\n",
    "        updated_w1= self.w1 - self.learning_rate * grad_w1\n",
    "        updated_w2 = self.w2 - self.learning_rate * grad_w2\n",
    "        \n",
    "        return updated_w1,updated_w2\n",
    "        \n",
    "            \n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(0, len(self.x_train), self.batch_size):\n",
    "                x_batch = self.x_train[i:i+self.batch_size]\n",
    "                y_batch = self.y_train[i:i+self.batch_size]\n",
    "                \n",
    "                hidden_layer_input, hidden_layer_output, scores = self.forward(x_batch)\n",
    "                loss = self.loss(y_batch, scores)\n",
    "                \n",
    "                updated_w1,updated_w2 = self.backward(hidden_layer_input, hidden_layer_output, scores, y_batch,x_batch)\n",
    "                self.w1 = updated_w1\n",
    "                self.w2 = updated_w2\n",
    "        \n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {loss}\")\n",
    "\n",
    "    def test(self,x_test, y_test):\n",
    "        hidden_layer_input = x_test.dot(self.w1)\n",
    "        hidden_layer_output = 1 / (1 + np.exp(-hidden_layer_input))\n",
    "        output_layer_input = hidden_layer_output.dot(self.w2) \n",
    "        predicted_labels = np.argmax(output_layer_input, axis=1)\n",
    "        \n",
    "        accuracy = np.mean(predicted_labels == y_test)\n",
    "        print(f\"Accuracy on test set: {accuracy}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6f8551f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 32*32*3\n",
    "hidden_size = 256\n",
    "output_size = 10\n",
    "learning_rate = 0.08\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fda37c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 521.6798093085264\n",
      "Epoch 2/10, Loss: 422.7967696026113\n",
      "Epoch 3/10, Loss: 360.9669021138811\n",
      "Epoch 4/10, Loss: 316.910186904673\n",
      "Epoch 5/10, Loss: 281.8931528859355\n",
      "Epoch 6/10, Loss: 253.88620032181865\n",
      "Epoch 7/10, Loss: 231.46129196308289\n",
      "Epoch 8/10, Loss: 213.58588919176515\n",
      "Epoch 9/10, Loss: 199.10188220429865\n",
      "Epoch 10/10, Loss: 187.33107979829987\n"
     ]
    }
   ],
   "source": [
    "# sigmoid without regularization\n",
    "NN1 = NNSig_without_reg(x_train, y_train)\n",
    "NN1.setHyperParameters(input_size, hidden_size, output_size, learning_rate, epochs)\n",
    "NN1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8d2b5aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.2824\n"
     ]
    }
   ],
   "source": [
    "NN1.test(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae98139",
   "metadata": {},
   "source": [
    "#### tanh activation function without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e1bdfa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tanh without regularization\n",
    "class NNtanh_without_reg:\n",
    "    \n",
    "    def __init__(self, x_train,y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = 128\n",
    "        \n",
    "    def setHyperParameters(self, input_size, hidden_size, output_size, learning_rate, epochs):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_size = input_size\n",
    "        self.w1 = np.random.randn(input_size, hidden_size)\n",
    "        self.w2 = np.random.randn(hidden_size, output_size)\n",
    "        self.bias1 = np.zeros((1, hidden_size))\n",
    "        self.bias2 = np.zeros((1, output_size))\n",
    "\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        hidden_layer_input = x_batch.dot(self.w1) + self.bias1\n",
    "        hidden_layer_output = np.tanh(hidden_layer_input)\n",
    "        output_layer_input = hidden_layer_output.dot(self.w2) +self.bias2\n",
    "        scores = np.exp(output_layer_input - np.max(output_layer_input, axis=1, keepdims=True))\n",
    "\n",
    "        return hidden_layer_input, hidden_layer_output, scores\n",
    "\n",
    "    def loss(self,y_batch,scores):\n",
    "        correct_scores = scores[np.arange(len(scores)), y_batch]\n",
    "        loss = -np.sum(np.log(correct_scores / np.sum(scores, axis=1)))\n",
    "        return loss\n",
    "        \n",
    "    def backward(self, hidden_layer_input, hidden_layer_output, scores, y_batch,x_batch):\n",
    "        grad_scores = scores / np.sum(scores, axis=1, keepdims=True)\n",
    "        grad_scores[np.arange(len(grad_scores)), y_batch] -= 1\n",
    "        grad_scores /= len(grad_scores)\n",
    "        \n",
    "        grad_bias2 = np.sum(grad_scores, axis=0, keepdims=True)\n",
    "        grad_w2 = hidden_layer_output.T.dot(grad_scores)\n",
    "        grad_hidden = grad_scores.dot(self.w2.T)\n",
    "        \n",
    "        grad_hidden_layer_input = grad_hidden * (1 - np.tanh(hidden_layer_output)**2)\n",
    "        grad_w1 = x_batch.T.dot(grad_hidden_layer_input)\n",
    "        grad_bias1 = np.sum(grad_hidden_layer_input, axis=0, keepdims=True)\n",
    "\n",
    "        updated_w1= self.w1 - self.learning_rate * grad_w1\n",
    "        updated_w2 = self.w2 - self.learning_rate * grad_w2\n",
    "        updated_bias1 = self.bias1 - self.learning_rate * grad_bias1\n",
    "        updated_bias2 = self.bias2 - self.learning_rate * grad_bias2\n",
    "\n",
    "        return updated_w1,updated_w2,updated_bias1,updated_bias2\n",
    "        \n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(0, len(self.x_train), self.batch_size):\n",
    "                x_batch = self.x_train[i:i+self.batch_size]\n",
    "                y_batch = self.y_train[i:i+self.batch_size]\n",
    "                \n",
    "                hidden_layer_input, hidden_layer_output, scores = self.forward(x_batch)\n",
    "                loss = self.loss(y_batch, scores)\n",
    "                \n",
    "                updated_w1,updated_w2,updated_bias1,updated_bias2 = self.backward(hidden_layer_input, hidden_layer_output, scores, y_batch,x_batch)\n",
    "                self.w1 = updated_w1\n",
    "                self.w2 = updated_w2\n",
    "                self.bias1 = updated_bias1\n",
    "                self.bias2 = updated_bias2\n",
    "        \n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {loss}\")\n",
    "\n",
    "    def test(self,x_test, y_test):\n",
    "        hidden_layer_input = x_test.dot(self.w1) + self.bias1\n",
    "        hidden_layer_output = 1 / (1 + np.exp(-hidden_layer_input))\n",
    "        output_layer_input = hidden_layer_output.dot(self.w2) + self.bias2 \n",
    "        predicted_labels = np.argmax(output_layer_input, axis=1)\n",
    "        \n",
    "        accuracy = np.mean(predicted_labels == y_test)\n",
    "        print(f\"Accuracy on test set: {accuracy}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bdf49e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters \n",
    "input_size = 32*32*3\n",
    "hidden_size = 256\n",
    "output_size = 10\n",
    "learning_rate = 0.08\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9d5c9bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 767.5716038496046\n",
      "Epoch 2/10, Loss: 619.2820759638171\n",
      "Epoch 3/10, Loss: 496.89237308706413\n",
      "Epoch 4/10, Loss: 381.23148674566517\n",
      "Epoch 5/10, Loss: 277.81531790675\n",
      "Epoch 6/10, Loss: 206.72043248373927\n",
      "Epoch 7/10, Loss: 169.7846781925635\n",
      "Epoch 8/10, Loss: 155.36255408214942\n",
      "Epoch 9/10, Loss: 152.02328155772304\n",
      "Epoch 10/10, Loss: 151.78284486187283\n",
      "Accuracy on test set: 0.2382\n"
     ]
    }
   ],
   "source": [
    "# tanh without regularization\n",
    "NN2 = NNtanh_without_reg(x_train, y_train)\n",
    "NN2.setHyperParameters(input_size, hidden_size, output_size, learning_rate, epochs)\n",
    "NN2.train()\n",
    "NN2.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dfc149",
   "metadata": {},
   "source": [
    "#### relu activation function without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1456c49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNReLU_without_reg:\n",
    "    \n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = 128\n",
    "        \n",
    "    def setHyperParameters(self, input_size, hidden_size, output_size, learning_rate, epochs):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_size = input_size\n",
    "        self.w1 = np.random.randn(input_size, hidden_size)\n",
    "        self.w2 = np.random.randn(hidden_size, output_size)\n",
    "        self.bias1 = np.zeros((1, hidden_size))\n",
    "        self.bias2 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        hidden_layer_input = x_batch.dot(self.w1) + self.bias1\n",
    "        hidden_layer_output = np.maximum(0, hidden_layer_input)  \n",
    "        output_layer_input = hidden_layer_output.dot(self.w2) + self.bias2\n",
    "        scores = np.exp(output_layer_input - np.max(output_layer_input, axis=1, keepdims=True))\n",
    "\n",
    "        return hidden_layer_input, hidden_layer_output, scores\n",
    "\n",
    "    def loss(self, y_batch, scores):\n",
    "        correct_scores = scores[np.arange(len(scores)), y_batch]\n",
    "        loss = -np.sum(np.log(correct_scores / np.sum(scores, axis=1)))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, hidden_layer_input, hidden_layer_output, scores, y_batch, x_batch):\n",
    "        grad_scores = scores / np.sum(scores, axis=1, keepdims=True)\n",
    "        grad_scores[np.arange(len(grad_scores)), y_batch] -= 1\n",
    "        grad_scores /= len(grad_scores)\n",
    "\n",
    "        grad_bias2 = np.sum(grad_scores, axis=0, keepdims=True)\n",
    "        grad_w2 = hidden_layer_output.T.dot(grad_scores)\n",
    "        grad_hidden = grad_scores.dot(self.w2.T)\n",
    "\n",
    "        grad_hidden_layer_input = grad_hidden * (hidden_layer_output > 0).astype(int) \n",
    "        grad_w1 = x_batch.T.dot(grad_hidden_layer_input)\n",
    "        grad_bias1 = np.sum(grad_hidden_layer_input, axis=0, keepdims=True)\n",
    "\n",
    "        updated_w1 = self.w1 - self.learning_rate * grad_w1\n",
    "        updated_w2 = self.w2 - self.learning_rate * grad_w2\n",
    "        updated_bias1 = self.bias1 - self.learning_rate * grad_bias1\n",
    "        updated_bias2 = self.bias2 - self.learning_rate * grad_bias2\n",
    "\n",
    "        return updated_w1, updated_w2, updated_bias1, updated_bias2\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(0, len(self.x_train), self.batch_size):\n",
    "                x_batch = self.x_train[i:i+self.batch_size]\n",
    "                y_batch = self.y_train[i:i+self.batch_size]\n",
    "\n",
    "                hidden_layer_input, hidden_layer_output, scores = self.forward(x_batch)\n",
    "                loss = self.loss(y_batch, scores)\n",
    "\n",
    "                updated_w1, updated_w2, updated_bias1, updated_bias2 = self.backward(hidden_layer_input, hidden_layer_output, scores, y_batch, x_batch)\n",
    "                self.w1 = updated_w1\n",
    "                self.w2 = updated_w2\n",
    "                \n",
    "    def test(self,x_test, y_test):\n",
    "        hidden_layer_input = x_test.dot(self.w1) + self.bias1\n",
    "        hidden_layer_output = 1 / (1 + np.exp(-hidden_layer_input))\n",
    "        output_layer_input = hidden_layer_output.dot(self.w2) + self.bias2 \n",
    "        predicted_labels = np.argmax(output_layer_input, axis=1)\n",
    "        \n",
    "        accuracy = np.mean(predicted_labels == y_test)\n",
    "        print(f\"Accuracy on test set: {accuracy}\")\n",
    "        \n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c3586bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18604\\200485322.py:27: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.sum(np.log(correct_scores / np.sum(scores, axis=1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.2337\n"
     ]
    }
   ],
   "source": [
    "# relu without regularization\n",
    "NN3 = NNReLU_without_reg(x_train, y_train)\n",
    "NN3.setHyperParameters(input_size, hidden_size, output_size, learning_rate, epochs)\n",
    "NN3.train()\n",
    "NN3.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f228c0e9",
   "metadata": {},
   "source": [
    "#### sigmoid with l2 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6abcdcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNSigmoid_with_reg:\n",
    "    \n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = 128\n",
    "        \n",
    "    def setHyperParameters(self, input_size, hidden_size, output_size, learning_rate, epochs, l2_reg):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_size = input_size\n",
    "        self.w1 = np.random.randn(input_size, hidden_size)\n",
    "        self.w2 = np.random.randn(hidden_size, output_size)\n",
    "        self.bias1 = np.zeros((1, hidden_size))\n",
    "        self.bias2 = np.zeros((1, output_size))\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        hidden_layer_input = x_batch.dot(self.w1) + self.bias1\n",
    "        hidden_layer_output = self.sigmoid(hidden_layer_input)\n",
    "        output_layer_input = hidden_layer_output.dot(self.w2) + self.bias2\n",
    "        scores = np.exp(output_layer_input - np.max(output_layer_input, axis=1, keepdims=True))\n",
    "\n",
    "        return hidden_layer_input, hidden_layer_output, scores\n",
    "\n",
    "    def loss(self, y_batch, scores):\n",
    "        correct_scores = scores[np.arange(len(scores)), y_batch]\n",
    "        data_loss = -np.sum(np.log(correct_scores / np.sum(scores, axis=1)))\n",
    "\n",
    "        reg_loss = 0.5 * self.l2_reg * (np.sum(self.w1**2) + np.sum(self.w2**2))\n",
    "\n",
    "        loss = data_loss + reg_loss\n",
    "        return loss\n",
    "\n",
    "    def backward(self, hidden_layer_input, hidden_layer_output, scores, y_batch, x_batch):\n",
    "        grad_scores = scores / np.sum(scores, axis=1, keepdims=True)\n",
    "        grad_scores[np.arange(len(grad_scores)), y_batch] -= 1\n",
    "        grad_scores /= len(grad_scores)\n",
    "\n",
    "        grad_bias2 = np.sum(grad_scores, axis=0, keepdims=True)\n",
    "        grad_w2 = hidden_layer_output.T.dot(grad_scores)\n",
    "        grad_hidden = grad_scores.dot(self.w2.T)\n",
    "\n",
    "        grad_hidden_layer_input = grad_hidden * (hidden_layer_output * (1 - hidden_layer_output))\n",
    "        grad_w1 = x_batch.T.dot(grad_hidden_layer_input)\n",
    "        grad_bias1 = np.sum(grad_hidden_layer_input, axis=0, keepdims=True)\n",
    "\n",
    "        # Add L2 regularization terms to gradients\n",
    "        grad_w1 += self.l2_reg * self.w1\n",
    "        grad_w2 += self.l2_reg * self.w2\n",
    "\n",
    "        updated_w1 = self.w1 - self.learning_rate * grad_w1\n",
    "        updated_w2 = self.w2 - self.learning_rate * grad_w2\n",
    "        updated_bias1 = self.bias1 - self.learning_rate * grad_bias1\n",
    "        updated_bias2 = self.bias2 - self.learning_rate * grad_bias2\n",
    "\n",
    "        return updated_w1, updated_w2, updated_bias1, updated_bias2\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(0, len(self.x_train), self.batch_size):\n",
    "                x_batch = self.x_train[i:i+self.batch_size]\n",
    "                y_batch = self.y_train[i:i+self.batch_size]\n",
    "\n",
    "                hidden_layer_input, hidden_layer_output, scores = self.forward(x_batch)\n",
    "                loss = self.loss(y_batch, scores)\n",
    "\n",
    "                updated_w1, updated_w2, updated_bias1, updated_bias2 = self.backward(hidden_layer_input, hidden_layer_output, scores, y_batch, x_batch)\n",
    "                self.w1 = updated_w1\n",
    "                self.w2 = updated_w2\n",
    "                self.bias1 = updated_bias1\n",
    "                self.bias2 = updated_bias2\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {loss}\")\n",
    "\n",
    "    def test(self, x_test, y_test):\n",
    "        hidden_layer_input = x_test.dot(self.w1) + self.bias1\n",
    "        hidden_layer_output = self.sigmoid(hidden_layer_input)\n",
    "        output_layer_input = hidden_layer_output.dot(self.w2) + self.bias2\n",
    "        predicted_labels = np.argmax(output_layer_input, axis=1)\n",
    "\n",
    "        accuracy = np.mean(predicted_labels == y_test)\n",
    "        print(f\"Accuracy on test set: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0fafec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d7accd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 816.8334760285798\n",
      "Epoch 2/10, Loss: 707.7861243342388\n",
      "Epoch 3/10, Loss: 630.5791178341631\n",
      "Epoch 4/10, Loss: 569.97450275878\n",
      "Epoch 5/10, Loss: 518.3646845242467\n",
      "Epoch 6/10, Loss: 475.5875249878342\n",
      "Epoch 7/10, Loss: 441.0709563736639\n",
      "Epoch 8/10, Loss: 412.9553544705075\n",
      "Epoch 9/10, Loss: 389.4664998797439\n",
      "Epoch 10/10, Loss: 369.3752258170813\n",
      "Accuracy on test set: 0.3102\n"
     ]
    }
   ],
   "source": [
    "# sigmoid with l2 regularization\n",
    "NN4 = NNSigmoid_with_reg(x_train, y_train)\n",
    "NN4.setHyperParameters(input_size, hidden_size, output_size, learning_rate, epochs, l2_reg)\n",
    "NN4.train()\n",
    "NN4.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68763730",
   "metadata": {},
   "source": [
    "#### tanh with l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a6cf3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tanh with l2 regularization\n",
    "class NNTanh_with_reg:\n",
    "    \n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = 128\n",
    "        \n",
    "    def setHyperParameters(self, input_size, hidden_size, output_size, learning_rate, epochs, l2_reg):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_size = input_size\n",
    "        self.w1 = np.random.randn(input_size, hidden_size)\n",
    "        self.w2 = np.random.randn(hidden_size, output_size)\n",
    "        self.bias1 = np.zeros((1, hidden_size))\n",
    "        self.bias2 = np.zeros((1, output_size))\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        hidden_layer_input = x_batch.dot(self.w1) + self.bias1\n",
    "        hidden_layer_output = self.tanh(hidden_layer_input)\n",
    "        output_layer_input = hidden_layer_output.dot(self.w2) + self.bias2\n",
    "        scores = np.exp(output_layer_input - np.max(output_layer_input, axis=1, keepdims=True))\n",
    "\n",
    "        return hidden_layer_input, hidden_layer_output, scores\n",
    "\n",
    "    def loss(self, y_batch, scores):\n",
    "        correct_scores = scores[np.arange(len(scores)), y_batch]\n",
    "        data_loss = -np.sum(np.log(correct_scores / np.sum(scores, axis=1)))\n",
    "\n",
    "        # Calculate regularization loss\n",
    "        reg_loss = 0.5 * self.l2_reg * (np.sum(self.w1**2) + np.sum(self.w2**2))\n",
    "\n",
    "        # Total loss\n",
    "        loss = data_loss + reg_loss\n",
    "        return loss\n",
    "\n",
    "    def backward(self, hidden_layer_input, hidden_layer_output, scores, y_batch, x_batch):\n",
    "        grad_scores = scores / np.sum(scores, axis=1, keepdims=True)\n",
    "        grad_scores[np.arange(len(grad_scores)), y_batch] -= 1\n",
    "        grad_scores /= len(grad_scores)\n",
    "\n",
    "        grad_bias2 = np.sum(grad_scores, axis=0, keepdims=True)\n",
    "        grad_w2 = hidden_layer_output.T.dot(grad_scores)\n",
    "        grad_hidden = grad_scores.dot(self.w2.T)\n",
    "\n",
    "        grad_hidden_layer_input = grad_hidden * (1 - np.tanh(hidden_layer_output)**2)\n",
    "        grad_w1 = x_batch.T.dot(grad_hidden_layer_input)\n",
    "        grad_bias1 = np.sum(grad_hidden_layer_input, axis=0, keepdims=True)\n",
    "\n",
    "        # Add L2 regularization terms to gradients\n",
    "        grad_w1 += self.l2_reg * self.w1\n",
    "        grad_w2 += self.l2_reg * self.w2\n",
    "\n",
    "        updated_w1 = self.w1 - self.learning_rate * grad_w1\n",
    "        updated_w2 = self.w2 - self.learning_rate * grad_w2\n",
    "        updated_bias1 = self.bias1 - self.learning_rate * grad_bias1\n",
    "        updated_bias2 = self.bias2 - self.learning_rate * grad_bias2\n",
    "\n",
    "        return updated_w1, updated_w2, updated_bias1, updated_bias2\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(0, len(self.x_train), self.batch_size):\n",
    "                x_batch = self.x_train[i:i+self.batch_size]\n",
    "                y_batch = self.y_train[i:i+self.batch_size]\n",
    "\n",
    "                hidden_layer_input, hidden_layer_output, scores = self.forward(x_batch)\n",
    "                loss = self.loss(y_batch, scores)\n",
    "\n",
    "                updated_w1, updated_w2, updated_bias1, updated_bias2 = self.backward(hidden_layer_input, hidden_layer_output, scores, y_batch, x_batch)\n",
    "                self.w1 = updated_w1\n",
    "                self.w2 = updated_w2\n",
    "                self.bias1 = updated_bias1\n",
    "                self.bias2 = updated_bias2\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {loss}\")\n",
    "\n",
    "    def test(self, x_test, y_test):\n",
    "        hidden_layer_input = x_test.dot(self.w1) + self.bias1\n",
    "        hidden_layer_output = self.tanh(hidden_layer_input)\n",
    "        output_layer_input = hidden_layer_output.dot(self.w2) + self.bias2\n",
    "        predicted_labels = np.argmax(output_layer_input, axis=1)\n",
    "\n",
    "        accuracy = np.mean(predicted_labels == y_test)\n",
    "        print(f\"Accuracy on test set: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "95acfb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1185.6672603998968\n",
      "Epoch 2/10, Loss: 921.1351214379335\n",
      "Epoch 3/10, Loss: 728.0993826846379\n",
      "Epoch 4/10, Loss: 603.1279204840587\n",
      "Epoch 5/10, Loss: 503.6198008752178\n",
      "Epoch 6/10, Loss: 439.6306457028636\n",
      "Epoch 7/10, Loss: 402.0860815654384\n",
      "Epoch 8/10, Loss: 379.3155023120542\n",
      "Epoch 9/10, Loss: 363.56249971790294\n",
      "Epoch 10/10, Loss: 350.1965891948523\n",
      "Accuracy on test set: 0.3773\n"
     ]
    }
   ],
   "source": [
    "# tanh with l2 regularization\n",
    "NN5 = NNTanh_with_reg(x_train, y_train)\n",
    "NN5.setHyperParameters(input_size, hidden_size, output_size, learning_rate, epochs, l2_reg)\n",
    "NN5.train()\n",
    "NN5.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2020ec",
   "metadata": {},
   "source": [
    "#### relu with l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8781d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNReLU_with_reg:\n",
    "    \n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = 128\n",
    "        \n",
    "    def setHyperParameters(self, input_size, hidden_size, output_size, learning_rate, epochs, l2_reg):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_size = input_size\n",
    "        self.w1 = np.random.randn(input_size, hidden_size)\n",
    "        self.w2 = np.random.randn(hidden_size, output_size)\n",
    "        self.bias1 = np.zeros((1, hidden_size))\n",
    "        self.bias2 = np.zeros((1, output_size))\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        hidden_layer_input = x_batch.dot(self.w1) + self.bias1\n",
    "        hidden_layer_output = np.maximum(0, hidden_layer_input)  \n",
    "        output_layer_input = hidden_layer_output.dot(self.w2) + self.bias2\n",
    "        scores = np.exp(output_layer_input - np.max(output_layer_input, axis=1, keepdims=True))\n",
    "\n",
    "        return hidden_layer_input, hidden_layer_output, scores\n",
    "\n",
    "    def loss(self, y_batch, scores):\n",
    "        correct_scores = scores[np.arange(len(scores)), y_batch]\n",
    "        data_loss = -np.sum(np.log(correct_scores / np.sum(scores, axis=1)))\n",
    "\n",
    "        # Calculate regularization loss\n",
    "        reg_loss = 0.5 * self.l2_reg * (np.sum(self.w1**2) + np.sum(self.w2**2))\n",
    "\n",
    "        # Total loss\n",
    "        loss = data_loss + reg_loss\n",
    "        return loss\n",
    "\n",
    "    def backward(self, hidden_layer_input, hidden_layer_output, scores, y_batch, x_batch):\n",
    "        grad_scores = scores / np.sum(scores, axis=1, keepdims=True)\n",
    "        grad_scores[np.arange(len(grad_scores)), y_batch] -= 1\n",
    "        grad_scores /= len(grad_scores)\n",
    "\n",
    "        grad_bias2 = np.sum(grad_scores, axis=0, keepdims=True)\n",
    "        grad_w2 = hidden_layer_output.T.dot(grad_scores)\n",
    "        grad_hidden = grad_scores.dot(self.w2.T)\n",
    "\n",
    "        grad_hidden_layer_input = grad_hidden * (hidden_layer_output > 0).astype(int)  \n",
    "        grad_w1 = x_batch.T.dot(grad_hidden_layer_input)\n",
    "        grad_bias1 = np.sum(grad_hidden_layer_input, axis=0, keepdims=True)\n",
    "\n",
    "        grad_w1 += self.l2_reg * self.w1\n",
    "        grad_w2 += self.l2_reg * self.w2\n",
    "\n",
    "        updated_w1 = self.w1 - self.learning_rate * grad_w1\n",
    "        updated_w2 = self.w2 - self.learning_rate * grad_w2\n",
    "        updated_bias1 = self.bias1 - self.learning_rate * grad_bias1\n",
    "        updated_bias2 = self.bias2 - self.learning_rate * grad_bias2\n",
    "\n",
    "        return updated_w1, updated_w2, updated_bias1, updated_bias2\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(0, len(self.x_train), self.batch_size):\n",
    "                x_batch = self.x_train[i:i+self.batch_size]\n",
    "                y_batch = self.y_train[i:i+self.batch_size]\n",
    "\n",
    "                hidden_layer_input, hidden_layer_output, scores = self.forward(x_batch)\n",
    "                loss = self.loss(y_batch, scores)\n",
    "\n",
    "                updated_w1, updated_w2, updated_bias1, updated_bias2 = self.backward(hidden_layer_input, hidden_layer_output, scores, y_batch, x_batch)\n",
    "                self.w1 = updated_w1\n",
    "                self.w2 = updated_w2\n",
    "                self.bias1 = updated_bias1\n",
    "                self.bias2 = updated_bias2\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {loss}\")\n",
    "\n",
    "    def test(self, x_test, y_test):\n",
    "        hidden_layer_input = x_test.dot(self.w1) + self.bias1\n",
    "        hidden_layer_output = np.maximum(0, hidden_layer_input)  # ReLU activation\n",
    "        output_layer_input = hidden_layer_output.dot(self.w2) + self.bias2\n",
    "        predicted_labels = np.argmax(output_layer_input, axis=1)\n",
    "\n",
    "        accuracy = np.mean(predicted_labels == y_test)\n",
    "        print(f\"Accuracy on test set: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "64804feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18604\\1009011736.py:28: RuntimeWarning: divide by zero encountered in log\n",
      "  data_loss = -np.sum(np.log(correct_scores / np.sum(scores, axis=1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 5941.359796140371\n",
      "Epoch 2/10, Loss: 2540.908546562265\n",
      "Epoch 3/10, Loss: 2321.9349600633877\n",
      "Epoch 4/10, Loss: 2068.5472031319505\n",
      "Epoch 5/10, Loss: 1252.6562611694876\n",
      "Epoch 6/10, Loss: 1393.5854680134462\n",
      "Epoch 7/10, Loss: 932.458032173202\n",
      "Epoch 8/10, Loss: 852.3384562223658\n",
      "Epoch 9/10, Loss: 745.6948176707347\n",
      "Epoch 10/10, Loss: 709.2040789049197\n",
      "Accuracy on test set: 0.3151\n"
     ]
    }
   ],
   "source": [
    "# relu with l2 regularization\n",
    "NN6 = NNReLU_with_reg(x_train, y_train)\n",
    "NN6.setHyperParameters(input_size, hidden_size, output_size, learning_rate, epochs, l2_reg)\n",
    "NN6.train()\n",
    "NN6.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4b717",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning: increasing epoches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b4380932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters \n",
    "input_size = 32*32*3\n",
    "hidden_size = 256\n",
    "output_size = 10\n",
    "learning_rate = 0.08\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bc89d3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 603.1886408810551\n",
      "Epoch 2/25, Loss: 495.1802934236118\n",
      "Epoch 3/25, Loss: 425.1593838081214\n",
      "Epoch 4/25, Loss: 371.8896749309305\n",
      "Epoch 5/25, Loss: 331.0956603025419\n",
      "Epoch 6/25, Loss: 298.0086278467523\n",
      "Epoch 7/25, Loss: 270.51747098536913\n",
      "Epoch 8/25, Loss: 247.73035227359583\n",
      "Epoch 9/25, Loss: 228.95866491917994\n",
      "Epoch 10/25, Loss: 213.6184447116627\n",
      "Epoch 11/25, Loss: 201.1645956105323\n",
      "Epoch 12/25, Loss: 191.0938238201308\n",
      "Epoch 13/25, Loss: 182.98860279833542\n",
      "Epoch 14/25, Loss: 176.50207802868732\n",
      "Epoch 15/25, Loss: 171.3359962496503\n",
      "Epoch 16/25, Loss: 167.23950474800876\n",
      "Epoch 17/25, Loss: 163.99823623383907\n",
      "Epoch 18/25, Loss: 161.43067208115906\n",
      "Epoch 19/25, Loss: 159.38813924942352\n",
      "Epoch 20/25, Loss: 157.75392084822113\n",
      "Epoch 21/25, Loss: 156.43675706397968\n",
      "Epoch 22/25, Loss: 155.36314199002052\n",
      "Epoch 23/25, Loss: 154.47346787880778\n",
      "Epoch 24/25, Loss: 153.72426019924183\n",
      "Epoch 25/25, Loss: 153.09281759838126\n",
      "Accuracy on test set: 0.361\n"
     ]
    }
   ],
   "source": [
    "# sigmoid without regularization\n",
    "NN7=NNSig_without_reg(x_train, y_train)\n",
    "NN7.setHyperParameters(input_size, hidden_size, output_size, learning_rate, epochs)\n",
    "NN7.train()\n",
    "NN7.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f2eab57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 921.0140010538407\n",
      "Epoch 2/25, Loss: 810.1651472620395\n",
      "Epoch 3/25, Loss: 715.5557130643906\n",
      "Epoch 4/25, Loss: 636.8002219839682\n",
      "Epoch 5/25, Loss: 573.6088838885397\n",
      "Epoch 6/25, Loss: 521.7588307543392\n",
      "Epoch 7/25, Loss: 479.08294058688637\n",
      "Epoch 8/25, Loss: 443.83106323856657\n",
      "Epoch 9/25, Loss: 414.5679104603585\n",
      "Epoch 10/25, Loss: 390.0577196829006\n",
      "Epoch 11/25, Loss: 369.2623589449024\n",
      "Epoch 12/25, Loss: 351.3506931276166\n",
      "Epoch 13/25, Loss: 335.70586079557347\n",
      "Epoch 14/25, Loss: 321.89573542814003\n",
      "Epoch 15/25, Loss: 309.5778783624857\n",
      "Epoch 16/25, Loss: 298.4890564576995\n",
      "Epoch 17/25, Loss: 288.4454557837005\n",
      "Epoch 18/25, Loss: 279.3019083174471\n",
      "Epoch 19/25, Loss: 270.91364536385316\n",
      "Epoch 20/25, Loss: 263.1341593172675\n",
      "Epoch 21/25, Loss: 255.86675303905992\n",
      "Epoch 22/25, Loss: 249.07492195250074\n",
      "Epoch 23/25, Loss: 242.7380984091927\n",
      "Epoch 24/25, Loss: 236.82671273573544\n",
      "Epoch 25/25, Loss: 231.30221703564644\n",
      "Accuracy on test set: 0.3772\n"
     ]
    }
   ],
   "source": [
    "#sigmoid with regularization with increased epoches\n",
    "NN8 = NNSigmoid_with_reg(x_train, y_train)\n",
    "NN8.setHyperParameters(input_size, hidden_size, output_size, learning_rate, epochs, l2_reg)\n",
    "NN8.train()\n",
    "NN8.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3a1a28cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 801.0696839982515\n",
      "Epoch 2/25, Loss: 555.232352050709\n",
      "Epoch 3/25, Loss: 432.6619317157538\n",
      "Epoch 4/25, Loss: 349.52687948002153\n",
      "Epoch 5/25, Loss: 256.6196766140171\n",
      "Epoch 6/25, Loss: 211.16707021186755\n",
      "Epoch 7/25, Loss: 175.9950748001419\n",
      "Epoch 8/25, Loss: 156.00196067436192\n",
      "Epoch 9/25, Loss: 149.3373900765412\n",
      "Epoch 10/25, Loss: 148.08143474887157\n",
      "Epoch 11/25, Loss: 146.71014141787393\n",
      "Epoch 12/25, Loss: 145.83349994033608\n",
      "Epoch 13/25, Loss: 145.26393389937007\n",
      "Epoch 14/25, Loss: 144.89259875208714\n",
      "Epoch 15/25, Loss: 144.7631465642074\n",
      "Epoch 16/25, Loss: 144.4762227718262\n",
      "Epoch 17/25, Loss: 144.46123994480615\n",
      "Epoch 18/25, Loss: 144.86009009803382\n",
      "Epoch 19/25, Loss: 145.19157265249027\n",
      "Epoch 20/25, Loss: 145.12619896287936\n",
      "Epoch 21/25, Loss: 145.05335898190572\n",
      "Epoch 22/25, Loss: 145.03037027525758\n",
      "Epoch 23/25, Loss: 145.2535019713668\n",
      "Epoch 24/25, Loss: 145.6685765815492\n",
      "Epoch 25/25, Loss: 146.0338958829936\n",
      "Accuracy on test set: 0.3116\n"
     ]
    }
   ],
   "source": [
    "#tanh without regularization with increased epoches\n",
    "NN9 = NNtanh_without_reg(x_train, y_train)\n",
    "NN9.setHyperParameters(input_size, hidden_size, output_size, learning_rate, epochs)\n",
    "NN9.train()\n",
    "NN9.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "64965979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 1365.1156762148453\n",
      "Epoch 2/25, Loss: 1066.9651156171242\n",
      "Epoch 3/25, Loss: 870.1194680560227\n",
      "Epoch 4/25, Loss: 699.4822778488323\n",
      "Epoch 5/25, Loss: 572.4516264644351\n",
      "Epoch 6/25, Loss: 488.115500132557\n",
      "Epoch 7/25, Loss: 435.86962963994847\n",
      "Epoch 8/25, Loss: 405.3119152359682\n",
      "Epoch 9/25, Loss: 385.85007534040983\n",
      "Epoch 10/25, Loss: 370.75232325579174\n",
      "Epoch 11/25, Loss: 356.86964542728197\n",
      "Epoch 12/25, Loss: 344.749678829413\n",
      "Epoch 13/25, Loss: 332.7297767347682\n",
      "Epoch 14/25, Loss: 320.5455500335621\n",
      "Epoch 15/25, Loss: 308.9985520649053\n",
      "Epoch 16/25, Loss: 298.71252105654395\n",
      "Epoch 17/25, Loss: 288.9732834432669\n",
      "Epoch 18/25, Loss: 279.56377853515085\n",
      "Epoch 19/25, Loss: 270.6644994301899\n",
      "Epoch 20/25, Loss: 262.0724638052869\n",
      "Epoch 21/25, Loss: 253.65902371561316\n",
      "Epoch 22/25, Loss: 246.0251242253342\n",
      "Epoch 23/25, Loss: 239.70472358483474\n",
      "Epoch 24/25, Loss: 234.24182212042732\n",
      "Epoch 25/25, Loss: 229.1857173206693\n",
      "Accuracy on test set: 0.3916\n"
     ]
    }
   ],
   "source": [
    "#tanh with regularization with increased epoches\n",
    "NN10 = NNTanh_with_reg(x_train, y_train)\n",
    "NN10.setHyperParameters(input_size, hidden_size, output_size, learning_rate, epochs, l2_reg)\n",
    "NN10.train()\n",
    "NN10.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e8b0dd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18604\\1009011736.py:28: RuntimeWarning: divide by zero encountered in log\n",
      "  data_loss = -np.sum(np.log(correct_scores / np.sum(scores, axis=1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 4420.789935737893\n",
      "Epoch 2/50, Loss: 2714.0658935789197\n",
      "Epoch 3/50, Loss: 2633.973520207864\n",
      "Epoch 4/50, Loss: 2122.8809281527665\n",
      "Epoch 5/50, Loss: 1820.3331728175542\n",
      "Epoch 6/50, Loss: 1318.3813425458927\n",
      "Epoch 7/50, Loss: 1277.2255099913802\n",
      "Epoch 8/50, Loss: 1254.8909645518568\n",
      "Epoch 9/50, Loss: 1214.0504648931992\n",
      "Epoch 10/50, Loss: 907.6388002164042\n",
      "Epoch 11/50, Loss: 889.3323750385523\n",
      "Epoch 12/50, Loss: 915.9865502861505\n",
      "Epoch 13/50, Loss: 914.6396532826826\n",
      "Epoch 14/50, Loss: 716.5623391687959\n",
      "Epoch 15/50, Loss: 837.1048975214585\n",
      "Epoch 16/50, Loss: 741.9251249424806\n",
      "Epoch 17/50, Loss: 658.3566483006866\n",
      "Epoch 18/50, Loss: 566.4429606520762\n",
      "Epoch 19/50, Loss: 595.3039442459561\n",
      "Epoch 20/50, Loss: 558.299351612564\n",
      "Epoch 21/50, Loss: 560.4279405160172\n",
      "Epoch 22/50, Loss: 481.3554024645289\n",
      "Epoch 23/50, Loss: 442.50925856141714\n",
      "Epoch 24/50, Loss: 469.54836964202616\n",
      "Epoch 25/50, Loss: 432.5463418933922\n",
      "Epoch 26/50, Loss: 412.06547406641334\n",
      "Epoch 27/50, Loss: 386.21507796858873\n",
      "Epoch 28/50, Loss: 367.30272780659243\n",
      "Epoch 29/50, Loss: 355.01636771398717\n",
      "Epoch 30/50, Loss: 355.1927648763682\n",
      "Epoch 31/50, Loss: 335.3969189378778\n",
      "Epoch 32/50, Loss: 323.66325382761204\n",
      "Epoch 33/50, Loss: 313.32588924328513\n",
      "Epoch 34/50, Loss: 301.4654972913625\n",
      "Epoch 35/50, Loss: 293.17042739381833\n",
      "Epoch 36/50, Loss: 282.1034767518248\n",
      "Epoch 37/50, Loss: 268.2486587872782\n",
      "Epoch 38/50, Loss: 268.7146629268773\n",
      "Epoch 39/50, Loss: 257.14375916980896\n",
      "Epoch 40/50, Loss: 254.39501081242042\n",
      "Epoch 41/50, Loss: 247.60619688947284\n",
      "Epoch 42/50, Loss: 242.032088986469\n",
      "Epoch 43/50, Loss: 236.56228868125174\n",
      "Epoch 44/50, Loss: 230.76284404784556\n",
      "Epoch 45/50, Loss: 225.64439861649691\n",
      "Epoch 46/50, Loss: 221.47587976830965\n",
      "Epoch 47/50, Loss: 217.59241355631468\n",
      "Epoch 48/50, Loss: 213.95296522409504\n",
      "Epoch 49/50, Loss: 210.19153442675872\n",
      "Epoch 50/50, Loss: 206.36161192094363\n",
      "Accuracy on test set: 0.4039\n"
     ]
    }
   ],
   "source": [
    "epochs= 50\n",
    "learning_rate = 0.04\n",
    "l2_reg = 0.001\n",
    "#relu with l2 with decreased learning rate and increased epoches\n",
    "NN11 = NNReLU_with_reg(x_train, y_train)\n",
    "NN11.setHyperParameters(input_size, hidden_size, output_size, learning_rate, epochs, l2_reg)\n",
    "NN11.train()\n",
    "NN11.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "36e83b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3034.8136139561575\n",
      "Epoch 2/50, Loss: 2344.5002763041675\n",
      "Epoch 3/50, Loss: 1827.6575599459845\n",
      "Epoch 4/50, Loss: 1435.013159162898\n",
      "Epoch 5/50, Loss: 1135.5056065621177\n",
      "Epoch 6/50, Loss: 906.3846090935845\n",
      "Epoch 7/50, Loss: 731.1867943643086\n",
      "Epoch 8/50, Loss: 597.2308019875381\n",
      "Epoch 9/50, Loss: 494.7710038635922\n",
      "Epoch 10/50, Loss: 416.2930796398062\n",
      "Epoch 11/50, Loss: 356.0824798535658\n",
      "Epoch 12/50, Loss: 309.7817301854162\n",
      "Epoch 13/50, Loss: 274.08686184656193\n",
      "Epoch 14/50, Loss: 246.50202743563148\n",
      "Epoch 15/50, Loss: 225.1290215650143\n",
      "Epoch 16/50, Loss: 208.54727025940693\n",
      "Epoch 17/50, Loss: 195.69432782786527\n",
      "Epoch 18/50, Loss: 185.71155346478255\n",
      "Epoch 19/50, Loss: 177.9052976925183\n",
      "Epoch 20/50, Loss: 171.7662768430538\n",
      "Epoch 21/50, Loss: 166.93228264461095\n",
      "Epoch 22/50, Loss: 163.131543742159\n",
      "Epoch 23/50, Loss: 160.14424019735014\n",
      "Epoch 24/50, Loss: 157.79108232880068\n",
      "Epoch 25/50, Loss: 155.93037056704895\n",
      "Epoch 26/50, Loss: 154.45238205351092\n",
      "Epoch 27/50, Loss: 153.27244449152488\n",
      "Epoch 28/50, Loss: 152.3249267508194\n",
      "Epoch 29/50, Loss: 151.55870667558622\n",
      "Epoch 30/50, Loss: 150.93383857109296\n",
      "Epoch 31/50, Loss: 150.4190447147723\n",
      "Epoch 32/50, Loss: 149.98983255000275\n",
      "Epoch 33/50, Loss: 149.62708808318368\n",
      "Epoch 34/50, Loss: 149.3159866028952\n",
      "Epoch 35/50, Loss: 149.04511294164584\n",
      "Epoch 36/50, Loss: 148.8057422130328\n",
      "Epoch 37/50, Loss: 148.59125437231467\n",
      "Epoch 38/50, Loss: 148.39665902835998\n",
      "Epoch 39/50, Loss: 148.2182098536948\n",
      "Epoch 40/50, Loss: 148.05309306130016\n",
      "Epoch 41/50, Loss: 147.89917990765534\n",
      "Epoch 42/50, Loss: 147.75483740344546\n",
      "Epoch 43/50, Loss: 147.61879154157884\n",
      "Epoch 44/50, Loss: 147.4900329840515\n",
      "Epoch 45/50, Loss: 147.36775165371725\n",
      "Epoch 46/50, Loss: 147.25128913227073\n",
      "Epoch 47/50, Loss: 147.1401034106848\n",
      "Epoch 48/50, Loss: 147.0337433262013\n",
      "Epoch 49/50, Loss: 146.93182958462071\n",
      "Epoch 50/50, Loss: 146.83403930635467\n",
      "Accuracy on test set: 0.4198\n"
     ]
    }
   ],
   "source": [
    "#sigmoid with regularization with increased epoches\n",
    "l2_reg = 0.008\n",
    "NN12 = NNSigmoid_with_reg(x_train, y_train)\n",
    "NN12.setHyperParameters(input_size, hidden_size, output_size, learning_rate, epochs, l2_reg)\n",
    "NN12.train()\n",
    "NN12.test(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
